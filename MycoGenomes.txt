### Step-by-step plan of Genera project
###############################################################################################################

### STEP1
###############################################################################################################
# Downloading the prokaryotes.txt file (in STEP1 folder)
wget https://ftp.ncbi.nlm.nih.gov/genomes/GENOME_REPORTS/prokaryotes.txt

# Extracting all the myco... species
grep 'Mycobacterium' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycolicibacter' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycolicibacterium' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycolicibacillus' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycobacteroides' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt

# The header line is added to the allmyco file
head -n 1 prokaryotes.txt > header
cat header 1b-allMyco_ncbigenome_overview.txt > 1c-allMyco_ncbigenome_overview2.txt

# Check the amount of entries --> 11423
wc -l > 1c-allMyco_ncbigenome_overview2.txt
 
# Using an R script to retain the assemblies with < 10 scaffolds (output: MycoGenomes.xlsx with 1336 genomes of (sub)species and sometimes multiple strains per species)
Run 2a-Scaffolds.R

# Since the species with their different strains were all kept in the list (MycoGenomes.xlsx) and couldn't get filtered in an automated way, 
# for each species that had multiple different strains the best quality strain was kept. Best quality was assessed by looking at 
# nr of scaffolds (pref <3), most recent date (= most up-to-date technique), status (ideally complete genome), ncbi search and checking 
# to see if the strain is in sari's list.
Output: MycoGenomesFiltered.xlsx (255 genomes)

# Getting the ftp url out of the filtered file so data can be downloaded with those URLs
Run 2b-ftpURL.R

# Next steps are done on the server in /mnt/DATA2/cami
mkdir STEP1
# Downloading all the sequences, moving the downloads to the correct folder and renaming folder/files using a shell script
# Start screen sesssion (exit session with ctrl+a d, reopen session with screen -r download)
cd STEP1
screen -S download
sh 3-down_move_rename.sh ftpURL.txt
# Some names were not completely correct, so manually changed with mv
# Then use sh rename2.sh to rename all the files correctly (in folder that was named incorrectly)



### STEP2
###############################################################################################################
# Preparing data for BUSCO
mkdir STEP2
cd STEP2
mkdir busco
cd busco
mkdir genomes
sh ../../4-busco_prep.sh

# Installing conda on server (in /home/cdedecker)
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
deactivate conda 
# log off from server and reconnect to initiate conda correctly
conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda

# Installing mamba as package manager since it is much faster (Enter > q > yes > yes)
wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh
bash Mambaforge-Linux-x86_64.sh

# Create conda environment and activate it
conda create --yes --name busco
conda activate busco
# Install BUSCO in the busco environment (takes a long time)
conda install -c conda-forge -c bioconda busco=5.4.7

# To be able to use --auto-lineage-prok you need sepp and pplacer installed --> didn't work, errors
# Just downloaded the bacteria_odb10 dataset (in /mnt/DATA2/cami/busco/genomes/)
busco --download bacteria_odb10

# Running BUSCO to check the quality of the assemblies
# Do the following step in the genomes folder (where you extracted the .fna.gz files)
for i in *.fna;do name=$(echo $i | sed 's/_genomic.fna//g');echo $name;busco -f -c 8 -m genome -i $i -o BUSCO-$name -l /mnt/DATA2/cami/busco/genomes/busco_downloads/lineages/bacteria_odb10/;done

# Problem: 
(File "/home/cdedecker/miniconda3/envs/busco/lib/python3.9/shutil.py", line 317, in copymode chmod_func(dst, stat.S_IMODE(st.st_mode))
PermissionError: [Errno 1] Operation not permitted: '/mnt/DATA2/cami/busco/genomes/BUSCO-TRY1/prodigal_output/predicted_genes/predicted.faa'
Cannot use chmod_func (file belongs to someone else -> root))
# Solution: run BUSCO locally (on laptop)

# Create busco conda environment and install it
conda create --yes --name busco
conda activate busco
conda install -c conda-forge -c bioconda busco=5.4.7
# Make busco and genomes folder
mkdir /home/guest/Traineeship/Data/busco
cd busco
mkdir genomes
cd genomes
# Download all the unzipped .fna files from server (using fileZilla)

# Download lineage dataset bacteria_odb10 (in /Traineeship/Data/busco/genomes)
busco --download bacteria_odb10
# Run busco locally
for i in *.fna;do name=$(echo $i | sed 's/_genomic.fna//g');echo $name;busco -f -c 8 -m genome -i $i -o BUSCO-$name -l /home/guest/Traineeship/Data/busco/genomes/busco_downloads/lineages/bacteria_odb10/;done

# Moving short_summary files to BUSCO-summaries directory  (in /Traineeship/Data/busco/genomes/)
mkdir /home/guest/Traineeship/Data/busco/BUSCO-summaries
for i in B*;do cd $i;cp *.txt ../../BUSCO_summaries ;cd ..;done

# Getting only the percentage out of the short_summary file (in /Traineeship/Data/busco/BUSCO_summaries)
sh ../../../Scripts/STEP2/4-after_busco.sh
# Open both the generated files (name.txt and percentage.txt) in excel/libreOffice and use space as delimiter
# There will be one row with 255 columns -> select the complete row > edit > cut > select the space A2 > right mouse click: transpose
# Paste columns in 1 file -> saved as BUSCO_percentage.xlsx

# Filter out all the rows that have a percentage < 95% and choose about one strain per subspecies -> saved in BUSCO_percentageFiltered.xlsx

# Preparing to make a barplot of the number of contigs per species by extracting the nr of contigs from the busco summaries
sh ../../../Scripts/STEP2/4-busco_contigs.sh
# Open both the generated files (name2.txt and contigs.txt) in libreOffice and use space as delimiter
# There will be one row with 255 columns -> select the complete row > edit > cut > select the space A2 > right mouse click: transpose
# Paste columns in 1 file -> saved as Contigs.xlsx
# The first name is wrong: Candidatus_Mycolicibacterium_alkanivorans.txt is now M_Mycolicibacterium_... --> changed it manually to Candidatus_M_alka...

# R-script to create histogram of number of contigs per species
Run 4-barplot_contigs.R



### STEP3
###############################################################################################################
# Preparing folder for STEP3 (in /mnt/DATA2/cami)
mkdir STEP3

# Creating conda environment and install panaroo
conda create --yes --name panaroo
mamba install -c conda-forge -c bioconda -c defaults panaroo

# Running Panaroo to cluster the data/peform msa (coregenome) on the server /mnt/DATA2/cami
mkdir /mnt/DATA2/cami/STEP3/panaroo
# Problem when testing locally: 
"Reading prokka input!" error -> need fasta seq in the gff file (the once donwloaded from ncbi don't have that)
# Solution: make a text file (input_file.txt) with a list with the path to the gff file and path to fasta file tab delimited
# panaroo -i input_file.txt -o ./panaroo_output --clean-mode strict --remove-invalid-genes --threads 25 -a core --core_threshold 1.0 -f 0.5

# Copying the needed files (.gff and .fna) to a folder on the server (/mnt/DATA2/cami/STEP2/busco/)
# Copy all the .gff files from the genbankFiles folder to /mnt/DATA2/cami/STEP3/panaroo
cd /mnt/DATA2/cami/STEP3/panaroo
sh ../5-panaroo_prep.sh
# Problem: not every species has a .gff file (so not every species is annotated)
# Solution: run prokka on all the files so they are annotated the same (prokka is used since the panaroo input wants prokka annotated files)

# Make new conda environment and install prokka
conda create --yes --name prokka
mamba install -c conda-forge -c bioconda -c defaults prokka

# Copy all .fna to genomes folder in /mnt/DATA2/cami/STEP3/prokka/genomes
mkdir /mnt/DATA2/cami/STEP3/prokka
cd prokka
mkdir genomes
cd ../../STEP2/busco/genomes/
cp *.fna ../../../STEP3/prokka/genomes
# Remove all .fna files that are not in the BUSCO_percentageFiltered (STEP2 folder) file
rm <53 files>

# Run prokka for all the .fna files (in /mnt/DATA2/cami/STEP3/prokka/genomes) (these are downloaded from NCBI)
for i in *.fna;do name=$(echo $i | sed 's/_genomic.fna//g');echo $name;prokka $i --cpus 8 --outdir prokka-$name --prefix $name;done
# Copy the .gff files from prokka output (/mnt/DATA2/cami/STEP3/prokka/genome) to /mnt/DATA2/cami/STEP3/panaroo
for i in prokka-*;do cp $i/*.gff ../../panaroo;done

# Run prokka for all the .fasta files (in /mnt/DATA2/cami/STEP3/prokka) (these are the 16 NTM genomes that they previously sequenced and assembled at ITM)
for i in *.fasta;do name=$(echo $i | sed 's/_final.fasta//g');echo $name;prokka $i --cpus 12 --outdir prokka-$name --prefix $name;done
# Copy the .gff files from prokka output (/mnt/DATA2/cami/STEP3/prokka) to /mnt/DATA2/cami/STEP3/panaroo
for i in prokka-*;do cp $i/*.gff ../panaroo;done

# Run panaroo on the prokka output .gff files 
conda activate panaroo
panaroo -i *.gff -o ./panaroo_output --clean-mode strict --remove-invalid-genes --threads 24 -a core --core_threshold 1.0 -f 0.5

# Copy the core_gene_alignment.aln to the raxml-ng folder
mkdir /mnt/DATA2/cami/STEP3/raxml-ng
cd /mnt/DATA2/cami/STEP3/panaroo/panaroo_output
cp core_gene_alignment.aln ../../raxml-ng

# Running RAxML-ng to build phylogenetic tree
# Creating conda environment to run RAxML-ng and install it
conda create --yes --name raxml-ng
mamba install -c bioconda raxml-ng

# Checking if the input file can be read (input = core_gene_alignment.aln = output from panaroo)
raxml-ng --check --msa core_gene_alignment.aln --model GTR+G --prefix MycoGeno
# This made a file that was called MycoGeno.raxml.reduced.phy

# Since file will be very big, best to do following command to create a binary MSA file (which can be loaded much faster than the other)
raxml-ng --parse --msa core_gene_alignment.aln --model GTR+G --prefix MycoGeno2
# 8 threads recommended
# Don't use this file, it will filter out all the identical genomes and we don't want them filtered out

# Running raxml-ng
raxml-ng --search --site-repeat on --msa MycoGeno.raxml.rba --prefix MycoGeno --threads 8 --model GTR+G 
## Since there were only 60 core genes when running Panaroo, the tree was done very quickly and not correct
## Solution = to use different tool SCARAP (and also try to rerun Panaroo but not annotating with Prokka before)


#########
# Using different tool than Panaroo, namely SCARAP
#########
cd /home/cdedecker
git clone https://github.com/SWittouck/SCARAP.git

# Make conda environment and install all dependencies (see github)
# Make alias in .bashrc file so you can just use scarap
nano .bashrc
(with the other aliases, type: alias scarap="/home/cdedecker/SCARAP-master/src/scarap/scarap.py")
source .bashrc
# Get all the .faa files in 1 folder (/mnt/DATA2/cami/STEP3/scarap_input)
# 9 genomes did not have a .faa file (total = 205, with ITM genomes)

# Run scarap in /mnt/DATA2/cami/STEP3
scarap core ./scarap_input ./scarap_output -t 4

# ERROR: gene names are not unique
# Might be because there are spaces in the gene names
# Possible solution: replace all spaces with _
cd scarap_input
for i in *;do sed -i 's/ /_/g' $i ;done
# Was not the solution 
# Change the name between [] at the end of each gene name to the name of the genome (command below is done manually for each file)
sed -i -E 's/\[(.*?)\]/[<correct species name between brackets>]/g' <name of .faa file>
# Solved the problem

# Run scarap again
scarap core ./scarap_input ./scarap_output -t 5

#### Not needed anymore
# Make one big alignment with concat function, use fna files too, so copy to a folder (scarap_input_fna)
scarap concat ./scarap_input ./scarap_output/genes.tsv ./supermatrix -n ./scarap_input_fna -t 5
# ERROR with genome name
# in the genes.tsv --> the name of the genomes were *_protein and it tries to search the corresponding .fna file (but that is named *_genomic.fna) --> so .faa and .fna files should be only name of genome
# Solution: change names from file
for i in *_protein.faa; do name=$(echo $i | sed 's/_protein.faa//g');mv $i $name.faa ;done
for i in *_genomic.fna; do name=$(echo $i | sed 's/_genomic.fna//g');mv $i $name.faa ;done
# Run scarap again
scarap core ./scarap_input ./scarap_output -t 5
#### Not needed anymore

# Run scarap concat to make alignment
scarap concat ./scarap_input ./scarap_output/genes.tsv ./supermatrix -t 5

# Copy supermatrix_aas.fasta to raxml-ng folder
raxml-ng --search --site-repeat on --msa supermatrix_aas.fasta --prefix MycoGenomes --threads 8 --model LG+G 


########
# Doing Panaroo again, but with the .gff files downloaded from ncbi (so no prokka)
# See if this changes things
########
# Copy all the genomic.gff and genomic.fna files to GenomesNCBI_gff folder
cd /home/guest/Traineeship/Data/GenomesNCBI_gff

# Using convert_refseq_to_prokka_gff.py script to get them in the correct format
for i in *.gff;do name=$(echo $i | sed 's/.gff//g');python ../../Scripts/STEP3/convert_refseq_to_prokka_gff.py -g $name.gff -f $name.fna -o $name.gff3;done
mv ./*.gff3 ../panaroo_input

# Run Panaroo in panaroo_input folder
cd ../panaroo_input
panaroo -i *.gff3 -o ../panaroo_output --clean-mode strict --remove-invalid-genes --threads 20 -a core --core_threshold 1.0 -f 0.5
# Gave only 66 core genes, still way to low (for reference SCARAP found 1413 core genes)
### Conclusion: won't be used! SCARAP will be used to make alignment for RAxML-ng



### STEP4
###############################################################################################################
# Preparing folder for STEP4 (in /mnt/DATA2/cami)
mkdir STEP4

## Seeing which NTM genomes contain genes that are involved in MA and DIM biosynthesis (genes annotated from M. tuberculosis h37Rv)
# Making blast db from Rv numbers found in Excel file (MA_DIM_genes.xlsx contains genes that are involved in MA and DIM biosynthesis)
# First make conda environment and install blast+
conda create --name blast
mamba install -c bioconda blast

# Search Rv numbers on Mycobrowser and download the sequences (done manually)
# Concatenate all sequences into one file
cat Rv* > MA_DIM.fasta
# New line on every > (done manually)

# Make own database with blast
makeblastdb -in MA_DIM.fasta -title MA_DIM_db -dbtype nucl -out MA_DIM -parse_seqids

# ERROR: Duplicate seq_ids are found: LCL|NC_000962.3 --> NC_000962.3 is present in all seq ids
# Solution: change NC_000692.3 to Rv id (file name)
for i in *.fasta;do name=$(echo $i | sed 's/.fasta//g');sed -i "0,/NC_000962\.3/s//$name/" $i ;done
# Remove previous big fasta file and make new one
cat *.fasta > MA_DIM.fasta
# New line on every > (done manually)

# Making blastdb
makeblastdb -in MA_DIM.fasta -title MA_DIM_db -dbtype nucl -out MA_DIM -parse_seqids

# Blast NTM genomes (removed all tuberculosis genomes) against the database
# For every downloaded genome from NCBI
for i in *.fna;do name=$(echo $i | sed 's/_genomic.fna//g');blastn -query $i -db db/MA_DIM -out $name.txt -evalue 1e-10;done
# For every ITM genome 
for i in *.fna;do name=$(echo $i | sed 's/.fna//g');blastn -query $i -db db/MA_DIM -out $name.txt -evalue 1e-10;done
# File contains score (bits) and E value --> high score (says something about the sequence similarity) and low E value (nr of expected hits of similar quality that could be found just by chance) is good

# On the server, all the data is in /mnt/DATA2/cami/STEP4/MA_DIM_genes
# There you have 3 folders:
#   Files: which are the Rv sequences and the big concatenated fasta file
#   db: which is the blast database made of the concatenated fasta file
#   Results: which contains the .txt files with the blast results for each NTM genome

# To only get the hits out of the txt files, use following commands
for i in *.txt;do name=$(echo $i | sed 's/.txt/_Result.txt/g') ;grep "^Rv" $i > $name ;done

# To create an Excel overview file of all the present/absent MA or DIM genes in the genomes, use MA_DIM_summary.py 


## Percentage horizontal gene transfer (HGT) per genome
# Data is from Conor and can be found at /mnt/DATA2/cami/HGT/NeoHGT_results/
# Get total number of predicted HGTs after refinement
for i in *_neoHGT;do grep "Total predicted HGTs after refinement:" $i/neoHGT_analyse.log | sed 's/Total predicted HGTs after refinement://g' | sed 's/\.//g' >> nr_pred_HGT.txt ; done
# Get total number of proteins
for i in *_neoHGT;do grep ":.*proteins" $i/neoHGT_analyse.log | sed 's/proteins.//g' | sed 's/.*://g' >> total_prot.txt ; done
# Paste data of both files in 1 excel file --> divide and multiply by 100 --> file is called Percentage_HGT.xlsx


### Using tools to find virulence factors, plasmids and prophages in NTM genomes
## PlasForest for the prediction of plasmids
# Installation of PlasForest 
mkdir PlasForestFolder
git clone https://github.com/leaemiliepradier/PlasForest.git 
# Installed dependencies in conda environment, see github for dependencies (https://github.com/leaemiliepradier/PlasForest)
mamba create --name plasforest
conda activate plasforest
# --> for scikit-learn=0.22.2.post1 do $ mamba install -c cctbx202003 scikit-learn

# Untar random forest classifier
tar -xzvf plasforest.sav.tar.gz

# Download database --> might ask you a valid e-mail address + might be some missing seq it wants to download, best to choose small (100 or 50) batch size
chmod 755 database_downloader.sh
./database_downloader.sh

# Test run it 
./test_plasforest.sh

# Running PlasForest on all files (from NCBI) --> you have to run the PlasForest.py script in the directory where the .sav file is in, otherwise it cannot find it
chmod 755 PlasForest.py
for i in ../Input_PlasForest/* ;do name=$(echo $i | sed 's/_genomic.fna/_Result.csv/g' | sed 's/..\/Input_PlasForest\///g') ; ./PlasForest.py -i ../Input_PlasForest/$i -o ../Output_PlasForest/$name --threads 4 ;done
# Running PlasForest on all files (from ITM)
for i in ../InputITM_PlasForest/* ;do name=$(echo $i | sed 's/.fna/_Result.csv/g' | sed 's/..\/InputITM_PlasForest\///g') ; ./PlasForest.py -i ../InputITM_PlasForest/$i -o ../Output_PlasForest/$name --threads 4 ;done

# To create a summary file of all results, use PlasForest_summary.py


## PlaSquid for the detection of plasmids
# Installation of PlaSquid
mkdir PlaSquidFolder
git clone https://github.com/mgimenez720/plaSquid.git 
# Installing dependencies in conda environement with .yml file
mamba env create -f environments/plaSquid.yml

# Run it on one sample
nextflow run main.nf --contigs ../../PlasForestFolder/Input_PlasForest/<file name> --outdir ../Output_plaSquid/
#! Kept on crashing, couldn't get past Minidist:Mapping
# Only trying subworkflow and not complete workflow
nextflow run main.nf --contigs ../../PlasForestFolder/Input_PlasForest/C* --outdir ../Output_plaSquid/ --minidist
#! Still crashed, couldn't get past Minidist:Mapping



