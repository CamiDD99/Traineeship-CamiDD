### Step-by-step plan of Genera project
###############################################################################################################

### STEP1
###############################################################################################################
# Downloading the prokaryotes.txt file (in STEP1 folder)
wget https://ftp.ncbi.nlm.nih.gov/genomes/GENOME_REPORTS/prokaryotes.txt

# Extracting all the myco... species
grep 'Mycobacterium' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycolicibacter' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycolicibacterium' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycolicibacillus' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt
grep 'Mycobacteroides' prokaryotes.txt >> 1b-allMyco_ncbigenome_overview.txt

# The header line is added to the allmyco file
head -n 1 prokaryotes.txt > header
cat header 1b-allMyco_ncbigenome_overview.txt > 1c-allMyco_ncbigenome_overview2.txt

# Check the amount of entries --> 11423
wc -l > 1c-allMyco_ncbigenome_overview2.txt
 
# Using an R script to retain the assemblies with < 10 scaffolds (output: MycoGenomes.xlsx with 1336 (sub)species (with strains))
Run 2a-Scaffolds.R

# Since the species with their different strains were all kept in the list (MycoGenomes.xlsx) and couldn't get filtered in an automated way, 
# for each species that had multiple different strains the best quality strain was kept. Best quality was assessed by looking at 
# nr of scaffolds (pref <3), most recent date (= most up-to-date technique), status (ideally complete genome), ncbi search and checking 
# to see if the strain is in sari's list.
Output: MycoGenomesFiltered.xlsx (255 (sub)species (with +-1 strain))

# Getting the ftp url out of the filtered file so data can be downloaded with those URLs
Run 2b-ftpURL.R

# Next steps are done on the server in /mnt/DATA2/cami
mkdir STEP1
# Downloading all the sequences, moving the downloads to the correct folder and renaming folder/files using a shell script
# Start screen sesssion (exit session with ctrl+a d, reopen session with screen -r download)
cd STEP1
screen -S download
sh 3-down_move_rename.sh ftpURL.txt
# Some names were not completely correct, so manually changed with mv
# Then use sh rename2.sh to rename all the files correctly (in folder w-that was named incorrectly)



### STEP2
###############################################################################################################
# Preparing data for BUSCO
mkdir STEP2
cd STEP2
mkdir busco
cd busco
mkdir genomes
sh ../../4-busco_prep.sh

# Installing conda on server (in /home/cdedecker)
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
deactivate conda 
# log off from server and reconnect to initiate conda correctly
conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda

# Installing mamba as package manager since it is much faster (Enter > q > yes > yes)
wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh
bash Mambaforge-Linux-x86_64.sh

# Create conda environment and activate it
conda create --yes --name busco
conda activate busco
# Install BUSCO in the busco environment (takes a long time)
conda install -c conda-forge -c bioconda busco=5.4.7

# To be able to use --auto-lineage-prok you need sepp and pplacer installed --> didn't work, errors
conda install -c bioconda sepp
# Just downloaded the bacteria_odb10 dataset (in /mnt/DATA2/cami/busco/genomes/)
busco --download bacteria_odb10

# Running BUSCO to check the quality of the assemblies
# See links for information on BUSCO: 
# https://busco.ezlab.org/busco_userguide.html#running-busco 
# https://ucdavis-bioinformatics-training.github.io/2020-Genome_Assembly_Workshop/busco/busco
# https://busco-archive.ezlab.org/v1/files/README.html 
# busco -f -c 8 -m [mode] -i [sequence file]  -o [output file] -l [lineage]
# -f = force overwriting of results files from a previous run with the same name
# -c = integer Number of CPU threads to be used (default: 1)
# -i = input file with sequences -> prob also all the genomic.fna files
# -l = lineage dataset (bacteria_odb10, or automatically with --auto-lineage-prok)
# -o = specify the name of the folder that will contain the output 
# -m = mode can be genome, proteins, transcriptome -> in this case genome since assessing quality of genomes

# Do the following step in the genomes folder (where you extracted the .fna.gz files)
for i in *.fna;do name=$(echo $i | sed 's/_genomic.fna//g');echo $name;busco -f -c 8 -m genome -i $i -o BUSCO-$name -l /mnt/DATA2/cami/busco/genomes/busco_downloads/lineages/bacteria_odb10/;done

# Problem: 
(File "/home/cdedecker/miniconda3/envs/busco/lib/python3.9/shutil.py", line 317, in copymode chmod_func(dst, stat.S_IMODE(st.st_mode))
PermissionError: [Errno 1] Operation not permitted: '/mnt/DATA2/cami/busco/genomes/BUSCO-TRY1/prodigal_output/predicted_genes/predicted.faa'
Cannot use chmod_func (file belongs to someone else -> root))
# Solution: run BUSCO locally (on laptop)

# Create busco conda environment and install it
conda create --yes --name busco
conda activate busco
conda install -c conda-forge -c bioconda busco=5.4.7
# Make busco and genomes folder
mkdir /home/guest/Traineeship/Data/busco
cd busco
mkdir genomes
cd genomes
# Download all the unzipped .fna files from server (using fileZilla)

# Download lineage dataset bacteria_odb10 (in /Traineeship/Data/busco/genomes)
busco --download bacteria_odb10
for i in *.fna;do name=$(echo $i | sed 's/_genomic.fna//g');echo $name;busco -f -c 8 -m genome -i $i -o BUSCO-$name -l /home/guest/Traineeship/Data/busco/genomes/busco_downloads/lineages/bacteria_odb10/;done

# Moving short_summary files to BUSCO-summaries directory  (in /Traineeship/Data/busco/genomes/)
mkdir /home/guest/Traineeship/Data/busco/BUSCO-summaries
for i in B*;do cd $i;cp *.txt ../../BUSCO_summaries ;cd ..;done

# Getting only the percentage out of the short_summary file (in /Traineeship/Data/busco/BUSCO_summaries)
sh ../../../Scripts/STEP2/4-after_busco.sh
# Open both the generated files (name.txt and percentage.txt) in excel/libreOffice and use space as delimiter
# There will be one row with 255 columns -> select the complete row > edit > cut > select the space A2 > right mouse click: transpose
# Paste columns in 1 file -> saved as BUSCO_percentage.xlsx

# Filter out all the rows that have a percentage < 95% and choose about one strain per subspecies -> saved in BUSCO_percentageFiltered.xlsx

# Making a histogram of the number of contigs per species by extracting the nr of contigs from the busco summaries
sh ../../../Scripts/STEP2/4-histo_contigs.sh
# Open both the generated files (name2.txt and contigs.txt) in libreOffice and use space as delimiter
# There will be one row with 255 columns -> select the complete row > edit > cut > select the space A2 > right mouse click: transpose
# Paste columns in 1 file -> saved as BUSCO_contigs.xlsx
# The first name is wrong: Candidatus_Mycolicibacterium_alkanivorans.txt is now M_Mycolicibacterium_... --> changed it manually to Candidatus_M_alka...

# Creating R-script to create histogram of number of contigs per species
Run 4-barplot_contigs.R


### STEP3
###############################################################################################################
# Creating conda environment and install panaroo
conda create --yes --name panaroo
mamba install -c conda-forge -c bioconda -c defaults panaroo

# Running Panaroo to cluster the data/peform msa (coregenome) on the server /mnt/DATA2/cami
mkdir /mnt/DATA2/cami/STEP3/panaroo
# Problem when testing locally: 
"Reading prokka input!" error -> need fasta seq in the gff file (the once donwloaded from ncbi don't have that)
# Solution: make a text file (input_file.txt) with a list with the path to the gff file and path to fasta file tab delimited
# panaroo -i input_file.txt -o ./panaroo_output --clean-mode strict --remove-invalid-genes --threads 25 -a core --core_threshold 1.0 -f 0.5

# Copying the needed files to a folder on the server (/mnt/DATA2/cami/STEP2/busco/)
# Copy all the .gff files from the genbankFiles folder to /mnt/DATA2/cami/STEP3/panaroo
cd /mnt/DATA2/cami/STEP3/panaroo
sh ../5-panaroo_prep.sh
# Problem: not every species has a .gff file (so not every species is annotated)
# Solution: run prokka on all the files so they are annotated the same (prokka is used since the panaroo input wants prokka annotated files)

# Make new conda environment and install prokka
conda create --yes --name prokka
mamba install -c conda-forge -c bioconda -c defaults prokka

# Copy all .fna to genomes folder in /mnt/DATA2/cami/STEP3/prokka/genomes
mkdir /mnt/DATA2/cami/STEP3/prokka
cd prokka
mkdir genomes
cd ../../STEP2/busco/genomes/
cp *.fna ../../../STEP3/prokka/genomes
# Remove all .fna files that are not in the BUSCO_percentageFiltered (STEP2 folder) file
rm <53 files>

# Run prokka for all the .fna files (in /mnt/DATA2/cami/STEP3/prokka/genomes) (these are downloaded from NCBI)
for i in *.fna;do name=$(echo $i | sed 's/_genomic.fna//g');echo $name;prokka $i --cpus 8 --outdir prokka-$name --prefix $name;done
# Copy the .gff files from prokka output (/mnt/DATA2/cami/STEP3/prokka/genome) to /mnt/DATA2/cami/STEP3/panaroo
for i in prokka-*;do cp $i/*.gff ../../panaroo;done

# Run prokka for all the .fasta files (in /mnt/DATA2/cami/STEP3/prokka) (these are from the 16 NTM genomes that they previously sequenced and assembled)
for i in *.fasta;do name=$(echo $i | sed 's/_final.fasta//g');echo $name;prokka $i --cpus 12 --outdir prokka-$name --prefix $name;done
# Copy the .gff files from prokka output (/mnt/DATA2/cami/STEP3/prokka) to /mnt/DATA2/cami/STEP3/panaroo
for i in prokka-*;do cp $i/*.gff ../panaroo;done

# Run panaroo on the prokka output .gff files 
conda activate panaroo
panaroo -i *.gff -o ./panaroo_output --clean-mode strict --remove-invalid-genes --threads 24 -a core --core_threshold 1.0 -f 0.5

# Copy the core_gene_alignment.aln to the raxml-ng folder
mkdir /mnt/DATA2/cami/STEP3/raxml-ng
cd /mnt/DATA2/cami/STEP3/panaroo/panaroo_output
cp core_gene_alignment.aln ../../raxml-ng

#########
# Using different tool, namely SCARAP
#########
cd /home/guest/Traineeship
git clone https://github.com/SWittouck/SCARAP.git

# Install all dependencies (see github)
# Make alias in .bashrc file so you can just use scarap
nano .bashrc
(with the other aliases, type: alias scarap="/home/guest/Traineeship/SCARAP/src/scarap/scarap.py")
source .bashrc
# Run scarap in /home/guest/Traineeship/Data
scarap core ./GenomesNCBI_faa ./core_output_scarap -t 4
# ERROR: gene names are not unique

########
# Doing Panaroo again, but with the .gff files downloaded from ncbi (so no prokka)
########
# Copy all the genomic.gff and genomic.fna files to AnnotatedNCBI_gff folder
cd /home/guest/Traineeship/Data/AnnotatedNCBI_gff

# Using convert_refseq_to_prokka_gff.py script to get them in the correct format
for i in *.gff;do name=$(echo $i | sed 's/.gff//g');python ../../Scripts/STEP3/convert_refseq_to_prokka_gff.py -g $name.gff -f $name.fna -o $name.gff3;done
mv ./*.gff3 ../panaroo_input

# Run Panaroo in panaroo_input folder
cd ../panaroo_input
panaroo -i *.gff3 -o ../panaroo_output --clean-mode strict --remove-invalid-genes --threads 24 -a core --core_threshold 1.0 -f 0.5

# Running RAxML-ng to build phylogenetic tree
# Creating conda environment to run RAxML-ng and install it
conda create --yes --name raxml-ng
mamba install -c bioconda raxml-ng

# Checking if the input file can be read (input = core_gene_alignment.aln = output from panaroo)
raxml-ng --check --msa core_gene_alignment.aln --model GTR+G --prefix MycoGeno
# This made a file that was called MycoGeno.raxml.reduced.phy

# Since file will be very big, best to do following command to create a binary MSA file (which can be loaded much faster than the other)
raxml-ng --parse --msa core_gene_alignment.aln --model GTR+G --prefix MycoGeno2
# 8 threads recommended

# Running raxml-ng
raxml-ng --search --site-repeat on --msa MycoGeno.raxml.rba --prefix MycoGeno --threads 8 --model GTR+G 
